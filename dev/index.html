<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · Llama2.jl</title><meta name="title" content="Home · Llama2.jl"/><meta property="og:title" content="Home · Llama2.jl"/><meta property="twitter:title" content="Home · Llama2.jl"/><meta name="description" content="Documentation for Llama2.jl."/><meta property="og:description" content="Documentation for Llama2.jl."/><meta property="twitter:description" content="Documentation for Llama2.jl."/><meta property="og:url" content="https://kleincode.github.io/Llama2.jl/"/><meta property="twitter:url" content="https://kleincode.github.io/Llama2.jl/"/><link rel="canonical" href="https://kleincode.github.io/Llama2.jl/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>Llama2.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Reference"><span>Reference</span></a></li></ul></li><li><a class="tocitem" href="example/">Getting Started</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/kleincode/Llama2.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/kleincode/Llama2.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Llama2.jl"><a class="docs-heading-anchor" href="#Llama2.jl">Llama2.jl</a><a id="Llama2.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Llama2.jl" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/kleincode/Llama2.jl">Llama2.jl</a>.</p><ul><li><a href="example/#Getting-Started">Getting Started</a></li><li class="no-marker"><ul><li><a href="example/#Generate-text">Generate text</a></li><li><a href="example/#Use-the-tokenizer">Use the tokenizer</a></li></ul></li></ul><h2 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h2><ul><li><a href="#Llama2.Config"><code>Llama2.Config</code></a></li><li><a href="#Llama2.ProbIndex"><code>Llama2.ProbIndex</code></a></li><li><a href="#Llama2.RunState"><code>Llama2.RunState</code></a></li><li><a href="#Llama2.Sampler"><code>Llama2.Sampler</code></a></li><li><a href="#Llama2.Sampler-Union{Tuple{Vector{T}}, Tuple{T}} where T&lt;:Real"><code>Llama2.Sampler</code></a></li><li><a href="#Llama2.Tokenizer"><code>Llama2.Tokenizer</code></a></li><li><a href="#Llama2.Transformer"><code>Llama2.Transformer</code></a></li><li><a href="#Llama2.TransformerWeights"><code>Llama2.TransformerWeights</code></a></li><li><a href="#Llama2.decode-Tuple{Tokenizer, Int64, Int64}"><code>Llama2.decode</code></a></li><li><a href="#Llama2.encode"><code>Llama2.encode</code></a></li><li><a href="#Llama2.forward!-Union{Tuple{T}, Tuple{Transformer{T}, Integer, Integer}} where T&lt;:Real"><code>Llama2.forward!</code></a></li><li><a href="#Llama2.generate-Union{Tuple{T}, Tuple{Transformer{T}, Tokenizer, Sampler{T}, String}} where T&lt;:Real"><code>Llama2.generate</code></a></li><li><a href="#Llama2.read_karpathy-Tuple{String}"><code>Llama2.read_karpathy</code></a></li><li><a href="#Llama2.read_karpathy_config-Tuple{IOStream}"><code>Llama2.read_karpathy_config</code></a></li><li><a href="#Llama2.read_karpathy_weights-Tuple{Config, IOStream}"><code>Llama2.read_karpathy_weights</code></a></li><li><a href="#Llama2.rmsnorm!-Union{Tuple{T}, Tuple{AbstractArray{T}, AbstractArray{T}, AbstractArray{T}}} where T&lt;:Real"><code>Llama2.rmsnorm!</code></a></li><li><a href="#Llama2.sample_argmax-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Real"><code>Llama2.sample_argmax</code></a></li><li><a href="#Llama2.sample_mult-Union{Tuple{T}, Tuple{AbstractVector{T}, T}} where T&lt;:Real"><code>Llama2.sample_mult</code></a></li><li><a href="#Llama2.sample_topp-Union{Tuple{T}, Tuple{AbstractVector{T}, T, T}} where T&lt;:Real"><code>Llama2.sample_topp</code></a></li><li><a href="#Llama2.softmax!-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T&lt;:Real"><code>Llama2.softmax!</code></a></li><li><a href="#Llama2.swiglu!-Union{Tuple{T}, Tuple{AbstractArray{T}, AbstractArray{T}}} where T&lt;:Real"><code>Llama2.swiglu!</code></a></li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.Config" href="#Llama2.Config"><code>Llama2.Config</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Config{T&lt;:Integer}</code></pre><p>Used to configure the initial parameters.</p><p><strong>Fields</strong></p><ul><li><p><code>dim::Integer</code>: Transformer Dimension</p></li><li><p><code>hidden_dim::Integer</code>: ffn Layers</p></li><li><p><code>n_layers::Integer</code>: Number of Layers</p></li><li><p><code>n_heads::Integer</code>: Number of Query Heads</p></li><li><p><code>n_kv_heads::Integer</code>: Number of key/value heads</p></li><li><p><code>vocab_size::Integer</code>: Vocabulary Size</p></li><li><p><code>seq_len::Integer</code>: Max Sequence Length</p></li></ul><p>Initializes parameters and checks for the correct dimensions.  For example, the config can be read from a file using the <a href="#Llama2.read_karpathy_config-Tuple{IOStream}"><code>read_karpathy_config</code></a> function and is part of the <a href="#Llama2.TransformerWeights"><code>TransformerWeights</code></a> function. </p><p>llama2.c correspondence Config (l.19)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/config.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.ProbIndex" href="#Llama2.ProbIndex"><code>Llama2.ProbIndex</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct ProbIndex{T&lt;:Real}</code></pre><p>Used when sorting probabilities during top-p sampling</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/sampler.jl#L3">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.RunState" href="#Llama2.RunState"><code>Llama2.RunState</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RunState{T&lt;:Real}</code></pre><p>State of the transformer model. The matrices are modified during a forward pass. It should never be necessary to manually modify this. While some of these arrays preserve actual neccessary state, some of them serve as preallocated buffers to speed up computation in the <a href="#Llama2.forward!-Union{Tuple{T}, Tuple{Transformer{T}, Integer, Integer}} where T&lt;:Real"><code>forward!</code></a> method.</p><p><strong>Fields</strong></p><ul><li><p><code>x::Vector{T} where T&lt;:Real</code>: Activations at current time stamp. Shape: (dim,)</p></li><li><p><code>xb::Vector{T} where T&lt;:Real</code>: Activations at current time stamp inside a residual branch. Shape: (dim,)</p></li><li><p><code>xb2::Vector{T} where T&lt;:Real</code>: An additional activation buffer for convenience. Shape: (dim,)</p></li><li><p><code>hb::Vector{T} where T&lt;:Real</code>: Buffer for the hidden dimension in the feed-forward net. Shape: (hidden_dim,)</p></li><li><p><code>hb2::Vector{T} where T&lt;:Real</code>: Buffer for the hidden dimension in the feed-forward net. Shape: (hidden_dim,)</p></li><li><p><code>q::Vector{T} where T&lt;:Real</code>: Stores the query vector in the attention part. Shape: (n<em>heads * head</em>size,)</p></li><li><p><code>att::Matrix{T} where T&lt;:Real</code>: Buffer for the attention scores. Shape: (n<em>heads, seq</em>len)</p></li><li><p><code>logits::Vector{T} where T&lt;:Real</code>: The output logits. Shape: (vocab_size,)</p></li><li><p><code>key_cache::Array{T, 3} where T&lt;:Real</code>: Cache for all the keys in the attention part. Shape: (n<em>kv</em>heads * head<em>size, seq</em>len, n_layers)</p></li><li><p><code>value_cache::Array{T, 3} where T&lt;:Real</code>: Cache for all the values in the attention part. Shape: (n<em>kv</em>heads * head<em>size, seq</em>len, n_layers)</p></li></ul><p>llama2.c correspondence: RunState (l. 50)</p><p><strong>Allocate from config</strong></p><pre><code class="nohighlight hljs">function RunState(config::Config) where {T&lt;:Real}</code></pre><p>Initializes the matrices in RunState based on the shapes provided in the Config.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/transformer.jl#L80">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.Sampler" href="#Llama2.Sampler"><code>Llama2.Sampler</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Sampler{T&lt;:Real}</code></pre><pre><code class="nohighlight hljs">Sampler()
function Sampler{T}(temperature::T, topp::T, rng_seed::Integer) where {T&lt;:Real}</code></pre><p>Used to return a sampled token (index) based on given logits. Depending on the parameters, the sampler supports greedy argmax, multinomial, or top-p sampling. It is recommended to either adjust the temperature or top-p to a non-default value but not both since they do similar things (constrain the sampling).</p><p><strong>Fields</strong></p><ul><li><p><code>temperature::Real</code>: Logits are divided by this value. A higher temperature value makes the output more diverse while a lower temperature makes the output more deterministic, converging to greedy argmax sampling at 0.</p></li><li><p><code>topp::Real</code>: Used for top-p sampling. Only consider the set of most likely tokens whose probabilities sum up to this value. If this is 0 or 1, no top-p sampling is used. For other values, this prevents less likely tokens from being sampled.</p></li><li><p><code>rng_state::Random.MersenneTwister</code></p></li></ul><p>llama2.c correspondence: Sampler (l. 577 - 715)</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; sampler_mult = Sampler{Float64}(0.5, 0.0, 1)
Sampler{Float64}(0.5, 0.0, Random.MersenneTwister(1))

julia&gt; [sampler_mult([-0.5, 0.5, 0.2]) for i in 1:10]
10-element Vector{Int64}:
 2
 2
 2
 1
 2
 2
 3
 3
 2
 3

julia&gt; sampler_det = Sampler{Float64}(0.0, 0.0, 1)
Sampler{Float64}(0.0, 0.0, Random.MersenneTwister(42))

julia&gt; [sampler_det([-0.5, 0.5, 0.2]) for i in 1:10]
10-element Vector{Int64}:
 2
 2
 2
 2
 2
 2
 2
 2
 2
 2

julia&gt; sampler_topp = Sampler{Float64}(1.0, 0.5, 1)
Sampler{Float64}(1.0, 0.5, Random.MersenneTwister(1))

julia&gt; [sampler_topp([-0.5, 0.5, 0.2]) for i in 1:10]
10-element Vector{Int64}:
 2
 2
 2
 2
 2
 2
 3
 3
 2
 3</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/sampler.jl#L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.Sampler-Union{Tuple{Vector{T}}, Tuple{T}} where T&lt;:Real" href="#Llama2.Sampler-Union{Tuple{Vector{T}}, Tuple{T}} where T&lt;:Real"><code>Llama2.Sampler</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Sample the next token id based on the logits.</p><p>The sampling strategy is selected based on the <code>temperature</code> and <code>topp</code> parameters of the <a href="#Llama2.Sampler"><code>Sampler</code></a>:</p><ul><li>If <code>temperature == 0</code>, always take the token with the highest probability (greedy argmax sampling), see <a href="#Llama2.sample_argmax-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Real"><code>sample_argmax</code></a>.</li><li>If <code>topp</code> is 0 or 1, apply the temperature to the logits and sample from the predicted probability distribution (multinomial sampling), see <a href="#Llama2.sample_mult-Union{Tuple{T}, Tuple{AbstractVector{T}, T}} where T&lt;:Real"><code>sample_mult</code></a>.</li><li>Otherwise, only sample from the smallest set of most likely tokens whose probabilities sum up to at least <code>topp</code> (top-p sampling), see <a href="#Llama2.sample_topp-Union{Tuple{T}, Tuple{AbstractVector{T}, T, T}} where T&lt;:Real"><code>sample_topp</code></a>. The temperature is still applied before.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/sampler.jl#L97">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.Tokenizer" href="#Llama2.Tokenizer"><code>Llama2.Tokenizer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Tokenizer{T&lt;:Real}</code></pre><p>Used for mapping from strings to token arrays (Int vectors) and back.</p><p><strong>Fields</strong></p><ul><li><p><code>index_to_token::Vector{String}</code>: Maps a token index to its string representation, for decoding</p></li><li><p><code>token_to_index::Dict{String, Int64}</code>: Maps a token string to its token index, for encoding</p></li><li><p><code>vocab_scores::Vector{T} where T&lt;:Real</code>: Scores of individual tokens for encoding</p></li></ul><p>llama2.c correspondence: Tokenizer (l. 372)</p><ul><li>index<em>to</em>token = vocab</li><li>token<em>to</em>index = sorted_vocab</li><li>removed max<em>token</em>length (not required in Julia)</li><li>removed byte_pieces (not required in Julia)</li></ul><p><strong>Load from Karpathy bin file</strong></p><pre><code class="nohighlight hljs">Tokenizer(tokenizer_path::String, vocab_size::Int)</code></pre><p>Constructs a Tokenizer by loading the vocabulary from a file in the llama2.c format. The vocabulary size must be known from the config.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Tokenizer(&quot;bin/tokenizer/tokenizer.bin&quot;, 32000)
Tokenizer([&quot;&lt;unk&gt;&quot;, &quot;
&lt;s&gt;
&quot;, &quot;
&lt;/s&gt;
&quot;, &quot;&lt;0x00&gt;&quot;, &quot;&lt;0x01&gt;&quot;, &quot;&lt;0x02&gt;&quot;, &quot;&lt;0x03&gt;&quot;, &quot;&lt;0x04&gt;&quot;, &quot;&lt;0x05&gt;&quot;, &quot;&lt;0x06&gt;&quot;  …  &quot;ὀ&quot;, &quot;げ&quot;, &quot;べ&quot;, &quot;边&quot;, &quot;还&quot;, &quot;黃&quot;, &quot;왕&quot;, &quot;收&quot;, &quot;弘&quot;, &quot;给&quot;], Dict(&quot;âr&quot; =&gt; 28727, &quot; properly&quot; =&gt; 6285, &quot;chem&quot; =&gt; 14970, &quot; patients&quot; =&gt; 22070, &quot; Plan&quot; =&gt; 8403, &quot;&lt;0x2A&gt;&quot; =&gt; 46, &quot;рос&quot; =&gt; 10375, &quot;null&quot; =&gt; 4305, &quot;rę&quot; =&gt; 15387, &quot;ört&quot; =&gt; 21069…), Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  -31731.0, -31732.0, -31733.0, -31734.0, -31735.0, -31736.0, -31737.0, -31738.0, -31739.0, -31740.0])</code></pre><p>llama2.c correspondence: build_tokenizer (l. 385)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/tokenizer.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.Transformer" href="#Llama2.Transformer"><code>Llama2.Transformer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Transformer{T&lt;:Real}</code></pre><p>A transformer model, consisting of a config, weights, and a run state.</p><p><strong>Fields</strong></p><ul><li><p><code>config::Config</code>: Hyperparameters of the architecture</p></li><li><p><code>weights::TransformerWeights</code>: Weights of the module</p></li><li><p><code>state::RunState</code>: Buffers for the wave of activations in the forward pass</p></li></ul><p>llama2.c correspondence: Transformer (l. 67)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/transformer.jl#L143">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.TransformerWeights" href="#Llama2.TransformerWeights"><code>Llama2.TransformerWeights</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct TransformerWeights{T&lt;:Real}</code></pre><pre><code class="nohighlight hljs">function TransformerWeights(config::Config) where {T&lt;:Real}</code></pre><p>Holds the weights for the Llama2 transformer model.</p><p><strong>Fields</strong></p><ul><li><p><code>token_embedding_table::Matrix{T} where T&lt;:Real</code>: Token embedding table: Mapping from token index to embedding vector. Shape: (dim, vocab_size)</p></li><li><p><code>rms_att_weight::Matrix{T} where T&lt;:Real</code>: Weights for rmsnorm before the attention for each layer. Shape: (dim, n_layers)</p></li><li><p><code>rms_ffn_weight::Matrix{T} where T&lt;:Real</code>: Weights for rmsnorm before the feed-forward net for each layer. Shape: (dim, n_layers)</p></li><li><p><code>wq::Array{T, 3} where T&lt;:Real</code>: Query weights for each attention layer. Shape: (n<em>heads * head</em>size, dim, n_layers)</p></li><li><p><code>wk::Array{T, 3} where T&lt;:Real</code>: Key weights for each attention layer. Shape: (dim, kv<em>dim, n</em>layers)</p></li><li><p><code>wv::Array{T, 3} where T&lt;:Real</code>: Value weights for each attention layer. Shape: (dim, kv<em>dim, n</em>layers)</p></li><li><p><code>wo::Array{T, 3} where T&lt;:Real</code>: Output weights for each attention layer. Shape: (n<em>heads * head</em>size, dim, n_layers)</p></li><li><p><code>w1::Array{T, 3} where T&lt;:Real</code>: First weight matrix for each feed forward layer (in -&gt; hidden). Shape: (dim, hidden<em>dim, n</em>layers)</p></li><li><p><code>w2::Array{T, 3} where T&lt;:Real</code>: Second weight matrix for each feed forward layer (hidden -&gt; out). Shape: (hidden<em>dim, dim, n</em>layers)</p></li><li><p><code>w3::Array{T, 3} where T&lt;:Real</code>: Third weight matrix for each feed forward layer (in -&gt; hidden). Shape: (dim, hidden<em>dim, n</em>layers)</p></li><li><p><code>rms_final_weight::Vector{T} where T&lt;:Real</code>: Weights for the final rmsnorm before the optional classifier head. Shape: (dim,)</p></li><li><p><code>wcls::Matrix{Float32}</code>: Weights for the optional classifier head. If there is no classifier (the usual case), this should equal token<em>embedding</em>table, translating embeddings back to logits. This is inspired by the original llama2.c implementation. Shape: (dim, vocab_size)</p></li></ul><p>llama2.c correspondence: TransformerWeights (l. 29)</p><p><strong>Allocate from config</strong></p><p>To create a new <code>TransformerWeights</code> instance with preallocated matrices, use the config constructor:</p><pre><code class="nohighlight hljs">function TransformerWeights(config::Config) where {T&lt;:Real}</code></pre><p>llama2.c correspondence: memory<em>map</em>weights (l. 111)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/transformer.jl#L4">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.decode-Tuple{Tokenizer, Int64, Int64}" href="#Llama2.decode-Tuple{Tokenizer, Int64, Int64}"><code>Llama2.decode</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">decode(
    tokenizer::Tokenizer,
    prev_token::Int64,
    token::Int64
) -&gt; String
</code></pre><p>Decodes a token index to a string. If the previous token is BOS (=2) and the token value starts with a leading space, the leading space is removed. Token indices are 1-based (different to the 0-based system in llama2.c).</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; [decode(tokenizer, 1, t) for t in [2, 15044, 3187, 29992]]
4-element Vector{String}:
 &quot;
&lt;s&gt;
&quot;
 &quot; Hello&quot;
 &quot; world&quot;
 &quot;!&quot;

julia&gt; decode(tokenizer, 1, 15044)
&quot; Hello&quot;

julia&gt; decode(tokenizer, 2, 15044) # BOS strips leading space
&quot;Hello&quot;</code></pre><p>llama2.c correspondence: decode (l. 418)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/tokenizer.jl#L167">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.encode" href="#Llama2.encode"><code>Llama2.encode</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">encode(tokenizer::Tokenizer, text::String) -&gt; Vector{Int64}
encode(
    tokenizer::Tokenizer,
    text::String,
    eos_token::Bool
) -&gt; Vector{Int64}
</code></pre><p>Encode a string text using a <a href="#Llama2.Tokenizer"><code>Tokenizer</code></a>.  An optional EOS token can be added. Encoded text can be decoded with the <a href="#Llama2.decode-Tuple{Tokenizer, Int64, Int64}"><code>decode</code></a> function.</p><p>Works by encoding each code unit as a single token, then iteratively merging them together according to the <a href="#Llama2.Tokenizer"><code>Tokenizer</code></a>&#39;s <code>vocab_scores</code>.</p><p>Note that token indices are 1-based (different to the 0-based system in the llama2.c).</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; encode(tokenizer, &quot;Hello world!&quot;)
4-element Vector{Int64}:
     2
 15044
  3187
 29992</code></pre><p>llama2.c correspondence: encode (l. 452)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/tokenizer.jl#L78">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.forward!-Union{Tuple{T}, Tuple{Transformer{T}, Integer, Integer}} where T&lt;:Real" href="#Llama2.forward!-Union{Tuple{T}, Tuple{Transformer{T}, Integer, Integer}} where T&lt;:Real"><code>Llama2.forward!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">forward!(
    transformer::Transformer{T&lt;:Real},
    token::Integer,
    pos::Integer
) -&gt; Vector{T} where T&lt;:Real
</code></pre><p>A single complete transformer forward pass for input token <code>token</code> at position <code>pos</code>, returning the output logits. <code>pos</code> is one-based, i.e. 1 &lt;= pos &lt;= seq_len. <code>token</code> is also a one-based token index. This modifies the RunState of the transformer.</p><p>llama2.c correspondence: forward (l. 231)</p><p><strong>Example</strong></p><p>To run token 5 at position 1 through the transformer and get the predicted output logits:</p><pre><code class="language-julia-repl hljs">julia&gt; forward!(transformer, 5, 1)
32000-element Vector{Float32}:
 -2.1009917
  1.664739
 -2.1005554
 -2.1007848
 -2.1005578
 -2.1009412
  ⋮
 -2.1007295
 -2.100759
 -2.1007874
 -2.1009996
 -2.1009269
 -2.1007652</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/transformer.jl#L162">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.generate-Union{Tuple{T}, Tuple{Transformer{T}, Tokenizer, Sampler{T}, String}} where T&lt;:Real" href="#Llama2.generate-Union{Tuple{T}, Tuple{Transformer{T}, Tokenizer, Sampler{T}, String}} where T&lt;:Real"><code>Llama2.generate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">generate(
    model::Transformer{T&lt;:Real},
    tokenizer::Tokenizer,
    sampler::Sampler{T&lt;:Real},
    prompt::String;
    verbose,
    display_output,
    display_prompt,
    max_steps
) -&gt; String
</code></pre><p>Generate a sequence based on a given language model, tokenizer, sampler and prompt.</p><p>There are several optional boolean flags:</p><ul><li><code>verbose::Bool</code>: Print the achieved tokens/s</li><li><code>display_output::Bool</code>: Print the output</li><li><code>display_prompt::Bool</code>: Print the prompt. Ignored if <code>display_output</code> is <code>false</code>.</li><li><code>max_steps::Int</code>: Maximum number of generation steps.</li></ul><p>llama2.c correspondence: generation loop (l. 729-783)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/generate.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.read_karpathy-Tuple{String}" href="#Llama2.read_karpathy-Tuple{String}"><code>Llama2.read_karpathy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">read_karpathy(
    file_path::String
) -&gt; Tuple{Config{Int32}, TransformerWeights{Float32}}
</code></pre><p>Reads a Karpathy file and returns the Config and Weights using the <a href="#Llama2.read_karpathy_config-Tuple{IOStream}"><code>read_karpathy_config</code></a> function and the <a href="#Llama2.read_karpathy_weights-Tuple{Config, IOStream}"><code>read_karpathy_weights</code></a> function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/read_karpathy.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.read_karpathy_config-Tuple{IOStream}" href="#Llama2.read_karpathy_config-Tuple{IOStream}"><code>Llama2.read_karpathy_config</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">read_karpathy_config(file::IOStream) -&gt; Config{Int32}
</code></pre><p>Read a <a href="#Llama2.Config"><code>Config</code></a> from a Karpathy model file.</p><p>llama2.c correspondence: read_config (l. 147)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/read_karpathy.jl#L44">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.read_karpathy_weights-Tuple{Config, IOStream}" href="#Llama2.read_karpathy_weights-Tuple{Config, IOStream}"><code>Llama2.read_karpathy_weights</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">read_karpathy_weights(
    config::Config,
    file::IOStream
) -&gt; TransformerWeights{Float32}
</code></pre><p>Read the weights of a Karpathy file and return them using the <a href="#Llama2.TransformerWeights"><code>TransformerWeights</code></a> function.</p><p>llama2.c correspondence: memory<em>map</em>weights (l. 111)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/read_karpathy.jl#L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.rmsnorm!-Union{Tuple{T}, Tuple{AbstractArray{T}, AbstractArray{T}, AbstractArray{T}}} where T&lt;:Real" href="#Llama2.rmsnorm!-Union{Tuple{T}, Tuple{AbstractArray{T}, AbstractArray{T}, AbstractArray{T}}} where T&lt;:Real"><code>Llama2.rmsnorm!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmsnorm!(
    o::AbstractArray{T&lt;:Real},
    x::AbstractArray{T&lt;:Real},
    weight::AbstractArray{T&lt;:Real}
)
</code></pre><p>Calculate the root mean square norm of a vector.  Reference in llama2.c lines 182-195</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/math_llama.jl#L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.sample_argmax-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Real" href="#Llama2.sample_argmax-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Real"><code>Llama2.sample_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sample_argmax(logits::AbstractArray{T&lt;:Real, 1}) -&gt; Any
</code></pre><p>Deterministically sample the token with the highest probability.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; sample_argmax([-0.5, 0.0, 0.5])
3</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/sampler.jl#L132">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.sample_mult-Union{Tuple{T}, Tuple{AbstractVector{T}, T}} where T&lt;:Real" href="#Llama2.sample_mult-Union{Tuple{T}, Tuple{AbstractVector{T}, T}} where T&lt;:Real"><code>Llama2.sample_mult</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sample_mult(
    probabilities::AbstractArray{T&lt;:Real, 1},
    coin::Real
) -&gt; Any
</code></pre><p>Sample index from a probability distribution (must sum to 1). Coin is a random number in [0, 1). Find the index that coin falls into.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; sample_mult([0.1, 0.2, 0.3, 0.4], 0.05)
1

julia&gt; sample_mult([0.1, 0.2, 0.3, 0.4], 0.15)
2

julia&gt; sample_mult([0.1, 0.2, 0.3, 0.4], 0.8)
4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/sampler.jl#L147">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.sample_topp-Union{Tuple{T}, Tuple{AbstractVector{T}, T, T}} where T&lt;:Real" href="#Llama2.sample_topp-Union{Tuple{T}, Tuple{AbstractVector{T}, T, T}} where T&lt;:Real"><code>Llama2.sample_topp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sample_topp(
    probabilities::AbstractArray{T&lt;:Real, 1},
    topp::Real,
    coin::Real
) -&gt; Any
</code></pre><p>Top-p sampling (or &quot;nucleus sampling&quot;) samples from the smallest set of tokens that exceed probability topp. This way we never sample tokens that have very low probabilities and are less likely to go &quot;off the rails&quot;. Coin is a random number in [0, 1).</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; sample_topp([0.1, 0.2, 0.3, 0.4], 1.0, 0.9)
1

julia&gt; sample_topp([0.1, 0.2, 0.3, 0.4], 0.5, 0.9)
3

julia&gt; sample_topp([0.1, 0.2, 0.3, 0.4], 0.4, 0.9)
3

julia&gt; sample_topp([0.1, 0.2, 0.3, 0.4], 0.39, 0.9)
4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/sampler.jl#L179">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.softmax!-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T&lt;:Real" href="#Llama2.softmax!-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T&lt;:Real"><code>Llama2.softmax!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">softmax!(x::AbstractArray{T&lt;:Real})
</code></pre><p>Calculate the softmax of a vector.  Reference in llama2.c lines 197-215</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/math_llama.jl#L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Llama2.swiglu!-Union{Tuple{T}, Tuple{AbstractArray{T}, AbstractArray{T}}} where T&lt;:Real" href="#Llama2.swiglu!-Union{Tuple{T}, Tuple{AbstractArray{T}, AbstractArray{T}}} where T&lt;:Real"><code>Llama2.swiglu!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">swiglu!(
    x::AbstractArray{T&lt;:Real},
    x2::AbstractArray{T&lt;:Real}
)
</code></pre><p>Activation function that combines GLU and Swish functions. </p><p class="math-container">\[swiglu(x, x_2) = x * x_2 * sigmoid(x)\]</p><p>Reference in llama2.c lines 338-345</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kleincode/Llama2.jl/blob/33bc27ccfb58912cbaefe8d2b0394e014b526d2e/src/math_llama.jl#L47">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="example/">Getting Started »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Monday 15 July 2024 18:14">Monday 15 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
