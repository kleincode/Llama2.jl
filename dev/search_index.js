var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = Llama2","category":"page"},{"location":"#Llama2.jl","page":"Home","title":"Llama2.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Llama2.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"example.md\"]","category":"page"},{"location":"#Reference","page":"Home","title":"Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [Llama2]","category":"page"},{"location":"#Llama2.Config","page":"Home","title":"Llama2.Config","text":"struct Config{T<:Integer}\n\nUsed to configure the initial parameters.\n\nFields\n\ndim::Integer: Transformer Dimension\nhidden_dim::Integer: ffn Layers\nn_layers::Integer: Number of Layers\nn_heads::Integer: Number of Query Heads\nn_kv_heads::Integer: Number of key/value heads\nvocab_size::Integer: Vocabulary Size\nseq_len::Integer: Max Sequence Length\n\nInitializes parameters and checks for the correct dimensions.  For example, the config can be read from a file using the read_karpathy_config function and is part of the TransformerWeights function. \n\nllama2.c correspondence Config (l.19)\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.ProbIndex","page":"Home","title":"Llama2.ProbIndex","text":"struct ProbIndex{T<:Real}\n\nUsed when sorting probabilities during top-p sampling\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.RunState","page":"Home","title":"Llama2.RunState","text":"mutable struct RunState{T<:Real}\n\nState of the transformer model. The matrices are modified during a forward pass. It should never be necessary to manually modify this. While some of these arrays preserve actual neccessary state, some of them serve as preallocated buffers to speed up computation in the forward! method.\n\nFields\n\nx::Vector{T} where T<:Real: Activations at current time stamp. Shape: (dim,)\nxb::Vector{T} where T<:Real: Activations at current time stamp inside a residual branch. Shape: (dim,)\nxb2::Vector{T} where T<:Real: An additional activation buffer for convenience. Shape: (dim,)\nhb::Vector{T} where T<:Real: Buffer for the hidden dimension in the feed-forward net. Shape: (hidden_dim,)\nhb2::Vector{T} where T<:Real: Buffer for the hidden dimension in the feed-forward net. Shape: (hidden_dim,)\nq::Vector{T} where T<:Real: Stores the query vector in the attention part. Shape: (nheads * headsize,)\natt::Matrix{T} where T<:Real: Buffer for the attention scores. Shape: (nheads, seqlen)\nlogits::Vector{T} where T<:Real: The output logits. Shape: (vocab_size,)\nkey_cache::Array{T, 3} where T<:Real: Cache for all the keys in the attention part. Shape: (nkvheads * headsize, seqlen, n_layers)\nvalue_cache::Array{T, 3} where T<:Real: Cache for all the values in the attention part. Shape: (nkvheads * headsize, seqlen, n_layers)\n\nllama2.c correspondence: RunState (l. 50)\n\nAllocate from config\n\nfunction RunState(config::Config) where {T<:Real}\n\nInitializes the matrices in RunState based on the shapes provided in the Config.\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.Sampler","page":"Home","title":"Llama2.Sampler","text":"struct Sampler{T<:Real}\n\nSampler()\nfunction Sampler{T}(temperature::T, topp::T, rng_seed::Integer) where {T<:Real}\n\nUsed to return a sampled token (index) based on given logits. Depending on the parameters, the sampler supports greedy argmax, multinomial, or top-p sampling. It is recommended to either adjust the temperature or top-p to a non-default value but not both since they do similar things (constrain the sampling).\n\nFields\n\ntemperature::Real: Logits are divided by this value. A higher temperature value makes the output more diverse while a lower temperature makes the output more deterministic, converging to greedy argmax sampling at 0.\ntopp::Real: Used for top-p sampling. Only consider the set of most likely tokens whose probabilities sum up to this value. If this is 0 or 1, no top-p sampling is used. For other values, this prevents less likely tokens from being sampled.\nrng_state::Random.MersenneTwister\n\nllama2.c correspondence: Sampler (l. 577 - 715)\n\nExample\n\njulia> sampler_mult = Sampler{Float64}(0.5, 0.0, 1)\nSampler{Float64}(0.5, 0.0, Random.MersenneTwister(1))\n\njulia> [sampler_mult([-0.5, 0.5, 0.2]) for i in 1:10]\n10-element Vector{Int64}:\n 2\n 2\n 2\n 1\n 2\n 2\n 3\n 3\n 2\n 3\n\njulia> sampler_det = Sampler{Float64}(0.0, 0.0, 1)\nSampler{Float64}(0.0, 0.0, Random.MersenneTwister(42))\n\njulia> [sampler_det([-0.5, 0.5, 0.2]) for i in 1:10]\n10-element Vector{Int64}:\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n\njulia> sampler_topp = Sampler{Float64}(1.0, 0.5, 1)\nSampler{Float64}(1.0, 0.5, Random.MersenneTwister(1))\n\njulia> [sampler_topp([-0.5, 0.5, 0.2]) for i in 1:10]\n10-element Vector{Int64}:\n 2\n 2\n 2\n 2\n 2\n 2\n 3\n 3\n 2\n 3\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.Sampler-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"Home","title":"Llama2.Sampler","text":"Sample the next token id based on the logits.\n\nThe sampling strategy is selected based on the temperature and topp parameters of the Sampler:\n\nIf temperature == 0, always take the token with the highest probability (greedy argmax sampling), see sample_argmax.\nIf topp is 0 or 1, apply the temperature to the logits and sample from the predicted probability distribution (multinomial sampling), see sample_mult.\nOtherwise, only sample from the smallest set of most likely tokens whose probabilities sum up to at least topp (top-p sampling), see sample_topp. The temperature is still applied before.\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.Tokenizer","page":"Home","title":"Llama2.Tokenizer","text":"struct Tokenizer{T<:Real}\n\nUsed for mapping from strings to token arrays (Int vectors) and back.\n\nFields\n\nindex_to_token::Vector{String}: Maps a token index to its string representation, for decoding\ntoken_to_index::Dict{String, Int64}: Maps a token string to its token index, for encoding\nvocab_scores::Vector{T} where T<:Real: Scores of individual tokens for encoding\n\nllama2.c correspondence: Tokenizer (l. 372)\n\nindextotoken = vocab\ntokentoindex = sorted_vocab\nremoved maxtokenlength (not required in Julia)\nremoved byte_pieces (not required in Julia)\n\nLoad from Karpathy bin file\n\nTokenizer(tokenizer_path::String, vocab_size::Int)\n\nConstructs a Tokenizer by loading the vocabulary from a file in the llama2.c format. The vocabulary size must be known from the config.\n\nExample\n\njulia> Tokenizer(\"bin/tokenizer/tokenizer.bin\", 32000)\nTokenizer([\"<unk>\", \"\n<s>\n\", \"\n</s>\n\", \"<0x00>\", \"<0x01>\", \"<0x02>\", \"<0x03>\", \"<0x04>\", \"<0x05>\", \"<0x06>\"  …  \"ὀ\", \"げ\", \"べ\", \"边\", \"还\", \"黃\", \"왕\", \"收\", \"弘\", \"给\"], Dict(\"âr\" => 28727, \" properly\" => 6285, \"chem\" => 14970, \" patients\" => 22070, \" Plan\" => 8403, \"<0x2A>\" => 46, \"рос\" => 10375, \"null\" => 4305, \"rę\" => 15387, \"ört\" => 21069…), Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  -31731.0, -31732.0, -31733.0, -31734.0, -31735.0, -31736.0, -31737.0, -31738.0, -31739.0, -31740.0])\n\nllama2.c correspondence: build_tokenizer (l. 385)\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.Transformer","page":"Home","title":"Llama2.Transformer","text":"struct Transformer{T<:Real}\n\nA transformer model, consisting of a config, weights, and a run state.\n\nFields\n\nconfig::Config: Hyperparameters of the architecture\nweights::TransformerWeights: Weights of the module\nstate::RunState: Buffers for the wave of activations in the forward pass\n\nllama2.c correspondence: Transformer (l. 67)\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.TransformerWeights","page":"Home","title":"Llama2.TransformerWeights","text":"struct TransformerWeights{T<:Real}\n\nfunction TransformerWeights(config::Config) where {T<:Real}\n\nHolds the weights for the Llama2 transformer model.\n\nFields\n\ntoken_embedding_table::Matrix{T} where T<:Real: Token embedding table: Mapping from token index to embedding vector. Shape: (dim, vocab_size)\nrms_att_weight::Matrix{T} where T<:Real: Weights for rmsnorm before the attention for each layer. Shape: (dim, n_layers)\nrms_ffn_weight::Matrix{T} where T<:Real: Weights for rmsnorm before the feed-forward net for each layer. Shape: (dim, n_layers)\nwq::Array{T, 3} where T<:Real: Query weights for each attention layer. Shape: (nheads * headsize, dim, n_layers)\nwk::Array{T, 3} where T<:Real: Key weights for each attention layer. Shape: (dim, kvdim, nlayers)\nwv::Array{T, 3} where T<:Real: Value weights for each attention layer. Shape: (dim, kvdim, nlayers)\nwo::Array{T, 3} where T<:Real: Output weights for each attention layer. Shape: (nheads * headsize, dim, n_layers)\nw1::Array{T, 3} where T<:Real: First weight matrix for each feed forward layer (in -> hidden). Shape: (dim, hiddendim, nlayers)\nw2::Array{T, 3} where T<:Real: Second weight matrix for each feed forward layer (hidden -> out). Shape: (hiddendim, dim, nlayers)\nw3::Array{T, 3} where T<:Real: Third weight matrix for each feed forward layer (in -> hidden). Shape: (dim, hiddendim, nlayers)\nrms_final_weight::Vector{T} where T<:Real: Weights for the final rmsnorm before the optional classifier head. Shape: (dim,)\nwcls::Matrix{Float32}: Weights for the optional classifier head. If there is no classifier (the usual case), this should equal tokenembeddingtable, translating embeddings back to logits. This is inspired by the original llama2.c implementation. Shape: (dim, vocab_size)\n\nllama2.c correspondence: TransformerWeights (l. 29)\n\nAllocate from config\n\nTo create a new TransformerWeights instance with preallocated matrices, use the config constructor:\n\nfunction TransformerWeights(config::Config) where {T<:Real}\n\nllama2.c correspondence: memorymapweights (l. 111)\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.decode-Tuple{Tokenizer, Int64, Int64}","page":"Home","title":"Llama2.decode","text":"decode(\n    tokenizer::Tokenizer,\n    prev_token::Int64,\n    token::Int64\n) -> String\n\n\nDecodes a token index to a string. If the previous token is BOS (=2) and the token value starts with a leading space, the leading space is removed. Token indices are 1-based (different to the 0-based system in llama2.c).\n\nExample\n\njulia> [decode(tokenizer, 1, t) for t in [2, 15044, 3187, 29992]]\n4-element Vector{String}:\n \"\n<s>\n\"\n \" Hello\"\n \" world\"\n \"!\"\n\njulia> decode(tokenizer, 1, 15044)\n\" Hello\"\n\njulia> decode(tokenizer, 2, 15044) # BOS strips leading space\n\"Hello\"\n\nllama2.c correspondence: decode (l. 418)\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.encode","page":"Home","title":"Llama2.encode","text":"encode(tokenizer::Tokenizer, text::String) -> Vector{Int64}\nencode(\n    tokenizer::Tokenizer,\n    text::String,\n    eos_token::Bool\n) -> Vector{Int64}\n\n\nEncode a string text using a Tokenizer.  An optional EOS token can be added. Encoded text can be decoded with the decode function.\n\nWorks by encoding each code unit as a single token, then iteratively merging them together according to the Tokenizer's vocab_scores.\n\nNote that token indices are 1-based (different to the 0-based system in the llama2.c).\n\nExample\n\njulia> encode(tokenizer, \"Hello world!\")\n4-element Vector{Int64}:\n     2\n 15044\n  3187\n 29992\n\nllama2.c correspondence: encode (l. 452)\n\n\n\n\n\n","category":"function"},{"location":"#Llama2.forward!-Union{Tuple{T}, Tuple{Transformer{T}, Integer, Integer}} where T<:Real","page":"Home","title":"Llama2.forward!","text":"forward!(\n    transformer::Transformer{T<:Real},\n    token::Integer,\n    pos::Integer\n) -> Vector{T} where T<:Real\n\n\nA single complete transformer forward pass for input token token at position pos, returning the output logits. pos is one-based, i.e. 1 <= pos <= seq_len. token is also a one-based token index. This modifies the RunState of the transformer.\n\nllama2.c correspondence: forward (l. 231)\n\nExample\n\nTo run token 5 at position 1 through the transformer and get the predicted output logits:\n\njulia> forward!(transformer, 5, 1)\n32000-element Vector{Float32}:\n -2.1009917\n  1.664739\n -2.1005554\n -2.1007848\n -2.1005578\n -2.1009412\n  ⋮\n -2.1007295\n -2.100759\n -2.1007874\n -2.1009996\n -2.1009269\n -2.1007652\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.generate-Union{Tuple{T}, Tuple{Transformer{T}, Tokenizer, Sampler{T}, String}, Tuple{Transformer{T}, Tokenizer, Sampler{T}, String, Bool}, Tuple{Transformer{T}, Tokenizer, Sampler{T}, String, Bool, Bool}, Tuple{Transformer{T}, Tokenizer, Sampler{T}, String, Bool, Bool, Bool}, Tuple{Transformer{T}, Tokenizer, Sampler{T}, String, Bool, Bool, Bool, Int64}} where T<:Real","page":"Home","title":"Llama2.generate","text":"generate(\n    model::Transformer{T<:Real},\n    tokenizer::Tokenizer,\n    sampler::Sampler{T<:Real},\n    prompt::String\n) -> String\ngenerate(\n    model::Transformer{T<:Real},\n    tokenizer::Tokenizer,\n    sampler::Sampler{T<:Real},\n    prompt::String,\n    verbose::Bool\n) -> String\ngenerate(\n    model::Transformer{T<:Real},\n    tokenizer::Tokenizer,\n    sampler::Sampler{T<:Real},\n    prompt::String,\n    verbose::Bool,\n    display_output::Bool\n) -> String\ngenerate(\n    model::Transformer{T<:Real},\n    tokenizer::Tokenizer,\n    sampler::Sampler{T<:Real},\n    prompt::String,\n    verbose::Bool,\n    display_output::Bool,\n    display_prompt::Bool\n) -> String\ngenerate(\n    model::Transformer{T<:Real},\n    tokenizer::Tokenizer,\n    sampler::Sampler{T<:Real},\n    prompt::String,\n    verbose::Bool,\n    display_output::Bool,\n    display_prompt::Bool,\n    max_steps::Int64\n) -> String\n\n\nGenerate a sequence based on a given language model, tokenizer, sampler and prompt.\n\nThere are several optional boolean flags:\n\nverbose::Bool: Print the achieved tokens/s\ndisplay_output::Bool: Print the output\ndisplay_prompt::Bool: Print the prompt. Ignored if display_output is false.\nmax_steps::Int: Maximum number of generation steps.\n\nllama2.c correspondence: generation loop (l. 729-783)\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.read_karpathy-Tuple{String}","page":"Home","title":"Llama2.read_karpathy","text":"read_karpathy(\n    file_path::String\n) -> Tuple{Config{Int32}, TransformerWeights{Float32}}\n\n\nReads a Karpathy file and returns the Config and Weights using the read_karpathy_config function and the read_karpathy_weights function.\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.read_karpathy_config-Tuple{IOStream}","page":"Home","title":"Llama2.read_karpathy_config","text":"read_karpathy_config(file::IOStream) -> Config{Int32}\n\n\nRead a Config from a Karpathy model file.\n\nllama2.c correspondence: read_config (l. 147)\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.read_karpathy_weights-Tuple{Config, IOStream}","page":"Home","title":"Llama2.read_karpathy_weights","text":"read_karpathy_weights(\n    config::Config,\n    file::IOStream\n) -> TransformerWeights{Float32}\n\n\nRead the weights of a Karpathy file and return them using the TransformerWeights function.\n\nllama2.c correspondence: memorymapweights (l. 111)\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.rmsnorm-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"Home","title":"Llama2.rmsnorm","text":"rmsnorm(\n    x::AbstractArray{T<:Real, 1},\n    weight::AbstractArray{T<:Real, 1}\n) -> Any\n\n\nCalculate the root mean square norm of a vector.  Reference in llama2.c lines 182-195\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.sample_argmax-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:Real","page":"Home","title":"Llama2.sample_argmax","text":"sample_argmax(logits::AbstractArray{T<:Real, 1}) -> Any\n\n\nDeterministically sample the token with the highest probability.\n\nExample\n\njulia> sample_argmax([-0.5, 0.0, 0.5])\n3\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.sample_mult-Union{Tuple{T}, Tuple{AbstractVector{T}, T}} where T<:Real","page":"Home","title":"Llama2.sample_mult","text":"sample_mult(\n    probabilities::AbstractArray{T<:Real, 1},\n    coin::Real\n) -> Any\n\n\nSample index from a probability distribution (must sum to 1). Coin is a random number in [0, 1). Find the index that coin falls into.\n\nExamples\n\njulia> sample_mult([0.1, 0.2, 0.3, 0.4], 0.05)\n1\n\njulia> sample_mult([0.1, 0.2, 0.3, 0.4], 0.15)\n2\n\njulia> sample_mult([0.1, 0.2, 0.3, 0.4], 0.8)\n4\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.sample_topp-Union{Tuple{T}, Tuple{AbstractVector{T}, T, T}} where T<:Real","page":"Home","title":"Llama2.sample_topp","text":"sample_topp(\n    probabilities::AbstractArray{T<:Real, 1},\n    topp::Real,\n    coin::Real\n) -> Any\n\n\nTop-p sampling (or \"nucleus sampling\") samples from the smallest set of tokens that exceed probability topp. This way we never sample tokens that have very low probabilities and are less likely to go \"off the rails\". Coin is a random number in [0, 1).\n\nExamples\n\njulia> sample_topp([0.1, 0.2, 0.3, 0.4], 1.0, 0.9)\n1\n\njulia> sample_topp([0.1, 0.2, 0.3, 0.4], 0.5, 0.9)\n3\n\njulia> sample_topp([0.1, 0.2, 0.3, 0.4], 0.4, 0.9)\n3\n\njulia> sample_topp([0.1, 0.2, 0.3, 0.4], 0.39, 0.9)\n4\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.softmax-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:Real","page":"Home","title":"Llama2.softmax","text":"softmax(x::AbstractArray{T<:Real, 1}) -> Any\n\n\nCalculate the softmax of a vector.  Reference in llama2.c lines 197-215\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.swiglu-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"Home","title":"Llama2.swiglu","text":"swiglu(\n    x::AbstractArray{T<:Real, 1},\n    x2::AbstractArray{T<:Real, 1}\n) -> Any\n\n\nActivation function that combines GLU and Swish functions. \n\nswiglu(x x_2) = x * x_2 * sigmoid(x)\n\nReference in llama2.c lines 338-345\n\n\n\n\n\n","category":"method"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"CurrentModule = Llama2","category":"page"},{"location":"example/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"The model weights can be downloaded from Andrew Karpathy's HuggingFace tinyllamas repo. The default tokenizer, which is compatible with stories110M.bin, stories15M.bin, and stories42M.bin, can be downloaded from our repository or from the llama2.c repository.","category":"page"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"Add the package to your local environment via Pkg by running","category":"page"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"add https://github.com/kleincode/Llama2.jl","category":"page"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"Import the package using","category":"page"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"julia> using Llama2","category":"page"},{"location":"example/#Generate-text","page":"Getting Started","title":"Generate text","text":"","category":"section"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"config, weights = read_karpathy(\"bin/transformer/stories15M.bin\") # replace with path to model weights\nstate = RunState{Float32}(config)\ntransformer = Transformer{Float32}(config, weights, state)\ntokenizer = Tokenizer(\"bin/tokenizer/tokenizer.bin\", config.vocab_size) # replace with path to tokenizer\nsampler = Sampler{Float32}(1.0f0, 0.9f0, 420)\n\nprompt = \"Once upon a\"\ngenerate(transformer, tokenizer, sampler, prompt, false)","category":"page"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"\"Once upon a time, there was a little girl named Lily. She loved to help her mommy in the kitchen. One day, her mommy was making some yummy cookies and asked Lily to help her. Lily was so excited! \\nShe put on her apron and stood on a stool so she could reach the cookies. Her mommy was so proud of her independent little girl. They mixed the ingredients together and put the cookies in the oven. \\nAfter a little while, the cookies were ready and they smelled delicious. Lily's mommy let her have a slice and they both enjoyed the warm, tasty cookie. From that day on, Lily loved to help her mommy in the kitchen and help cook yummy treats.\"","category":"page"},{"location":"example/#Use-the-tokenizer","page":"Getting Started","title":"Use the tokenizer","text":"","category":"section"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"julia> tokenizer = Tokenizer(\"bin/tokenizer/tokenizer.bin\", 32000)\nTokenizer{Float32}([\"<unk>\", \"\\n<s>\\n\", \"\\n</s>\\n\", \"<0x00>\", \"<0x01>\", \"<0x02>\", \"<0x03>\", \"<0x04>\", \"<0x05>\", \"<0x06>\"  …  \"ὀ\", \"げ\", \"べ\", \"边\", \"还\", \"黃\", \"왕\", \"收\", \"弘\", \"给\"], Dict(\"âr\" => 28727, \" properly\" => 6285, \"chem\" => 14970, \" patients\" => 22070, \" Plan\" => 8403, \"<0x2A>\" => 46, \"рос\" => 10375, \"null\" => 4305, \"rę\" => 15387, \"ört\" => 21069…), Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  -31731.0, -31732.0, -31733.0, -31734.0, -31735.0, -31736.0, -31737.0, -31738.0, -31739.0, -31740.0])\njulia> encode(tokenizer, \"Tokens are beautiful! :))\")\n8-element Vector{Int64}:\n     2\n 11891\n   576\n   527\n  9561\n 29992\n   585\n   877\njulia> decode(tokenizer, 2, 11891)\n\"Tok\"","category":"page"},{"location":"example/","page":"Getting Started","title":"Getting Started","text":"For more information, check out Tokenizer.","category":"page"}]
}
